<!DOCTYPE html>
<html lang="zh_cn">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"github.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="vMotion 相关文章Powered-off vMotion (relocate) of a large virtual machine across datastores may timeout in vCenterhttps:&#x2F;&#x2F;knowledge.broadcom.com&#x2F;external&#x2F;article&#x2F;318734&#x2F;poweredoff-vmotion-relocate-of-a-la">
<meta property="og:type" content="article">
<meta property="og:title" content="VMware-vMotion相关">
<meta property="og:url" content="http://github.com/rohman-zhu/posts/cf3a31ba/index.html">
<meta property="og:site_name" content="诺曼实验室">
<meta property="og:description" content="vMotion 相关文章Powered-off vMotion (relocate) of a large virtual machine across datastores may timeout in vCenterhttps:&#x2F;&#x2F;knowledge.broadcom.com&#x2F;external&#x2F;article&#x2F;318734&#x2F;poweredoff-vmotion-relocate-of-a-la">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blogs.vmware.com/vsphere/files/2020/03/vmotion-history-2048x463.png">
<meta property="og:image" content="https://blogs.vmware.com/vsphere/files/2020/03/vMotion-page-trace-v2-old.png">
<meta property="og:image" content="https://blogs.vmware.com/vsphere/files/2020/03/vMotion-page-trace-v2-new.png">
<meta property="og:image" content="https://blogs.vmware.com/vsphere/files/2020/03/pagetableoverviewv2.png">
<meta property="og:image" content="https://blogs.vmware.com/vsphere/files/2020/03/fullbitmapv2.png">
<meta property="og:image" content="http://blogs.vmware.com/vsphere/files/2020/03/compactedbitmapv2.png">
<meta property="og:image" content="https://blogs.vmware.com/vsphere/files/2020/03/vmotion7-improvements.png">
<meta property="article:published_time" content="2024-05-19T00:36:59.000Z">
<meta property="article:modified_time" content="2024-12-22T05:27:48.775Z">
<meta property="article:author" content="Rohman.Zhu">
<meta property="article:tag" content="vMotion">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blogs.vmware.com/vsphere/files/2020/03/vmotion-history-2048x463.png">


<link rel="canonical" href="http://github.com/rohman-zhu/posts/cf3a31ba/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh_cn","comments":true,"permalink":"http://github.com/rohman-zhu/posts/cf3a31ba/","path":"posts/cf3a31ba/","title":"VMware-vMotion相关"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>VMware-vMotion相关 | 诺曼实验室</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">诺曼实验室</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#vMotion-%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0"><span class="nav-number">1.</span> <span class="nav-text">vMotion 相关文章</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Powered-off-vMotion-relocate-of-a-large-virtual-machine-across-datastores-may-timeout-in-vCenter"><span class="nav-number">1.1.</span> <span class="nav-text">Powered-off vMotion (relocate) of a large virtual machine across datastores may timeout in vCenter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vMotion-%E5%90%8C%E6%97%B6%E8%BF%81%E7%A7%BB%E7%9A%84%E9%99%90%E5%88%B6"><span class="nav-number">1.2.</span> <span class="nav-text">vMotion 同时迁移的限制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiple-NIC-vMotion-in-vSphere"><span class="nav-number">1.3.</span> <span class="nav-text">Multiple-NIC vMotion in vSphere</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Troubleshooting-vMotion"><span class="nav-number">1.4.</span> <span class="nav-text">Troubleshooting vMotion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-Tune-vMotion-for-Lower-Migration-Times"><span class="nav-number">1.5.</span> <span class="nav-text">How to Tune vMotion for Lower Migration Times?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-is-Virtual-Memory-Translated-to-Physical-Memory"><span class="nav-number">1.6.</span> <span class="nav-text">How is Virtual Memory Translated to Physical Memory?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-vMotion-Process-Under-the-Hood"><span class="nav-number">1.7.</span> <span class="nav-text">The vMotion Process Under the Hood</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vSphere-7-vMotion-Enhancements"><span class="nav-number">1.8.</span> <span class="nav-text">vSphere 7 - vMotion Enhancements</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Large-VM-vMotion-Challenges"><span class="nav-number">1.8.1.</span> <span class="nav-text">Large VM vMotion Challenges</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Memory-Pre-copy-Optimizations"><span class="nav-number">1.8.2.</span> <span class="nav-text">Memory Pre-copy Optimizations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-we-do-it-in-vSphere-7"><span class="nav-number">1.8.3.</span> <span class="nav-text">How we do it in vSphere 7</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Page-Table-Granularity"><span class="nav-number">1.8.4.</span> <span class="nav-text">Page Table Granularity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Switch-over-Phase-Enhancements"><span class="nav-number">1.8.5.</span> <span class="nav-text">Switch-over Phase Enhancements</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-vSphere-7-does-it"><span class="nav-number">1.8.6.</span> <span class="nav-text">How vSphere 7 does it</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Performance-Improvements"><span class="nav-number">1.8.7.</span> <span class="nav-text">Performance Improvements</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#To-Conclude"><span class="nav-number">1.8.8.</span> <span class="nav-text">To Conclude</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vSphere-7-Storage-vMotion-Improvements"><span class="nav-number">1.9.</span> <span class="nav-text">vSphere-7-Storage vMotion Improvements</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ESXi%E6%8F%90%E5%8D%87vMotion%E9%80%9F%E7%8E%87-%EF%BC%8C-vMotion%E7%9A%84%E5%B1%9E%E6%80%A7%E8%AE%BE%E7%BD%AE"><span class="nav-number">1.10.</span> <span class="nav-text">ESXi提升vMotion速率 ， vMotion的属性设置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ESXi-%E5%B1%9E%E6%80%A7%E4%BF%A1%E6%81%AF"><span class="nav-number">1.10.1.</span> <span class="nav-text">ESXi 属性信息</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rohman.Zhu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">429</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">144</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh_cn">
    <link itemprop="mainEntityOfPage" href="http://github.com/rohman-zhu/posts/cf3a31ba/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rohman.Zhu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="诺曼实验室">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="VMware-vMotion相关 | 诺曼实验室">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          VMware-vMotion相关
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-05-19 00:36:59" itemprop="dateCreated datePublished" datetime="2024-05-19T00:36:59+00:00">2024-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-22 05:27:48" itemprop="dateModified" datetime="2024-12-22T05:27:48+00:00">2024-12-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/VMware/" itemprop="url" rel="index"><span itemprop="name">VMware</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/VMware/vSphere/" itemprop="url" rel="index"><span itemprop="name">vSphere</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="vMotion-相关文章"><a href="#vMotion-相关文章" class="headerlink" title="vMotion 相关文章"></a>vMotion 相关文章</h1><h2 id="Powered-off-vMotion-relocate-of-a-large-virtual-machine-across-datastores-may-timeout-in-vCenter"><a href="#Powered-off-vMotion-relocate-of-a-large-virtual-machine-across-datastores-may-timeout-in-vCenter" class="headerlink" title="Powered-off vMotion (relocate) of a large virtual machine across datastores may timeout in vCenter"></a>Powered-off vMotion (relocate) of a large virtual machine across datastores may timeout in vCenter</h2><p><a target="_blank" rel="noopener" href="https://knowledge.broadcom.com/external/article/318734/poweredoff-vmotion-relocate-of-a-large-v.html">https://knowledge.broadcom.com/external/article/318734/poweredoff-vmotion-relocate-of-a-large-v.html</a></p>
<h2 id="vMotion-同时迁移的限制"><a href="#vMotion-同时迁移的限制" class="headerlink" title="vMotion 同时迁移的限制"></a>vMotion 同时迁移的限制</h2><p><a target="_blank" rel="noopener" href="https://docs.vmware.com/cn/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-25EA5833-03B5-4EDD-A167-87578B8009B3.html#GUID-25EA5833-03B5-4EDD-A167-87578B8009B3">https://docs.vmware.com/cn/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-25EA5833-03B5-4EDD-A167-87578B8009B3.html#GUID-25EA5833-03B5-4EDD-A167-87578B8009B3</a></p>
<h2 id="Multiple-NIC-vMotion-in-vSphere"><a href="#Multiple-NIC-vMotion-in-vSphere" class="headerlink" title="Multiple-NIC vMotion in vSphere"></a>Multiple-NIC vMotion in vSphere</h2><p><a target="_blank" rel="noopener" href="https://knowledge.broadcom.com/external/article?legacyId=2007467">https://knowledge.broadcom.com/external/article?legacyId=2007467</a></p>
<h2 id="Troubleshooting-vMotion"><a href="#Troubleshooting-vMotion" class="headerlink" title="Troubleshooting vMotion"></a>Troubleshooting vMotion</h2><p><a target="_blank" rel="noopener" href="https://blogs.vmware.com/vsphere/2019/09/troubleshooting-vmotion.html">https://blogs.vmware.com/vsphere/2019/09/troubleshooting-vmotion.html</a></p>
<h2 id="How-to-Tune-vMotion-for-Lower-Migration-Times"><a href="#How-to-Tune-vMotion-for-Lower-Migration-Times" class="headerlink" title="How to Tune vMotion for Lower Migration Times?"></a>How to Tune vMotion for Lower Migration Times?</h2><p><a target="_blank" rel="noopener" href="https://blogs.vmware.com/vsphere/2019/09/how-to-tune-vmotion-for-lower-migration-times.html">https://blogs.vmware.com/vsphere/2019/09/how-to-tune-vmotion-for-lower-migration-times.html</a></p>
<h2 id="How-is-Virtual-Memory-Translated-to-Physical-Memory"><a href="#How-is-Virtual-Memory-Translated-to-Physical-Memory" class="headerlink" title="How is Virtual Memory Translated to Physical Memory?"></a>How is Virtual Memory Translated to Physical Memory?</h2><p><a target="_blank" rel="noopener" href="https://blogs.vmware.com/vsphere/2020/03/how-is-virtual-memory-translated-to-physical-memory.html">https://blogs.vmware.com/vsphere/2020/03/how-is-virtual-memory-translated-to-physical-memory.html</a></p>
<h2 id="The-vMotion-Process-Under-the-Hood"><a href="#The-vMotion-Process-Under-the-Hood" class="headerlink" title="The vMotion Process Under the Hood"></a>The vMotion Process Under the Hood</h2><p><a target="_blank" rel="noopener" href="https://blogs.vmware.com/vsphere/2019/07/the-vmotion-process-under-the-hood.html">https://blogs.vmware.com/vsphere/2019/07/the-vmotion-process-under-the-hood.html</a></p>
<h2 id="vSphere-7-vMotion-Enhancements"><a href="#vSphere-7-vMotion-Enhancements" class="headerlink" title="vSphere 7 - vMotion Enhancements"></a>vSphere 7 - vMotion Enhancements</h2><p>The vSphere vMotion feature enables customers to live-migrate workloads from source to destination ESXi hosts. Over time, we have developed vMotion to support new technologies. The vSphere 7 release is no exception to that, as we greatly improved the vMotion feature. The vMotion enhancements in vSphere 7 include a reduced performance impact during the live-migration and a reduced stun time. This blog post will go into details on how the vMotion improvements help customers to be comfortable using vMotion for large workloads.</p>
<p><img src="https://blogs.vmware.com/vsphere/files/2020/03/vmotion-history-2048x463.png" alt="vMotion发展"></p>
<p>To understand what we improved for vMotion in vSphere 7, it is imperative to understand the vMotion internals. Read the vMotion Process Under the Hood to learn more about the vMotion process itself.</p>
<h3 id="Large-VM-vMotion-Challenges"><a href="#Large-VM-vMotion-Challenges" class="headerlink" title="Large VM vMotion Challenges"></a>Large VM vMotion Challenges</h3><p>vSphere is the perfect platform to host large virtual machines (VMs) — also referred to as “Monster” VMs — with the current per VM resource maximums set to 256 vCPU’s and 6 TB of memory. However, we noticed that customers running workloads in large VMs were not always comfortable live-migrating these VMs. That was due to a potential impact on workload performance during the vMotion process, and a switch-over (stun) time that took too long.</p>
<p>For example, large transactional database platforms that are dealing with a substantial number of I&#x2F;Os could experience performance degradation during a vMotion, although the precise impact strongly depends on the workload’s characteristics &amp; sizing. The enhanced logic for vMotion in vSphere 7 overcomes all these challenges and allows us to live-migrate large workloads without significant impact on performance or availability.</p>
<h3 id="Memory-Pre-copy-Optimizations"><a href="#Memory-Pre-copy-Optimizations" class="headerlink" title="Memory Pre-copy Optimizations"></a>Memory Pre-copy Optimizations</h3><p>As explained in this video, we need to keep track of all the changed memory pages for a VM during its vMotion operation. Because it is a live-migration, the guest OS inside the VM will keep writing data to memory during the vMotion. We need to track and resend memory pages that are overwritten during a vMotion.</p>
<p><img src="https://blogs.vmware.com/vsphere/files/2020/03/vMotion-page-trace-v2-old.png" alt="vMotion "></p>
<p>The vMotion process installs a page tracer on all the vCPUs that are configured for the VM. By doing so, vMotion understands what memory pages are being overwritten. This is referred to as a ‘page fire’ during page tracing. We are distributing the tracing work to all the vCPU’s for the VM that is live-migrated.</p>
<p>To install the page tracer and to process a page fire, the vCPUs are briefly stopped. It’s only microseconds, but stopping all vCPUs disrupts the workload. Scaling up the compute resources of a VM increases the impact of a vMotion operation. After stopping the vCPUs for the tracing work, all memory Page Table Entries (PTE) are set to read-only, and the Translation Lookaside Buffers (TLB) are flushed to avoid TLB hits and force a page table walk so the vMotion process fully understands what memory pages have been overwritten. Learn more on these memory constructs in this blog</p>
<h3 id="How-we-do-it-in-vSphere-7"><a href="#How-we-do-it-in-vSphere-7" class="headerlink" title="How we do it in vSphere 7"></a>How we do it in vSphere 7</h3><p>The biggest impact comes from having to stop all the vCPUs for page tracing. What if we can install the page tracers without the need to stop all vCPUs? With vSphere 7, we are introducing the “Loose Page Trace Install.” The method for page tracing mostly remains the same but instead of using all vCPUs, we now only claim one vCPU to perform all the tracing work. All the other vCPUs that are entitled to the VM just continue to run the workload without interruption.</p>
<p><img src="https://blogs.vmware.com/vsphere/files/2020/03/vMotion-page-trace-v2-new.png" alt="vSphere 7 vmotion"></p>
<h3 id="Page-Table-Granularity"><a href="#Page-Table-Granularity" class="headerlink" title="Page Table Granularity"></a>Page Table Granularity</h3><p>So we reduced the cost of tracing, but what if we can make it even more efficient? We optimized the way we set memory to read-only, meaning there’s less work to be done on this part. Less work means increased efficiency. The way memory is set to read-only prior to vSphere 7 is on a 4KB page granularity. All the individual 4KB pages needed to be set to read-only access.</p>
<p><img src="https://blogs.vmware.com/vsphere/files/2020/03/pagetableoverviewv2.png" alt="Page"></p>
<p>As of vSphere 7, the Virtual Machine Monitor (VMM) process will set the read-only flag at a much larger granularity, on 1GB pages. If a page fire (a memory page is overwritten) occurs, the 1GB PTE is broken down into 2MB and 4KB pages. VMware engineers have seen that a VM is typically not touching all of its memory during a vMotion process. The memory working set size during a vMotion is typically only 10-30%. If more memory is used during the vMotion time, the cost efficiency will be less.</p>
<h3 id="Switch-over-Phase-Enhancements"><a href="#Switch-over-Phase-Enhancements" class="headerlink" title="Switch-over Phase Enhancements"></a>Switch-over Phase Enhancements</h3><p>All the enhancements we have discussed so far are done in the memory pre-copy phase, where the tracing happens. Once we reach memory convergence, meaning almost all memory is copied to the destination host, vMotion is ready to switch-over to the destination ESXi host. In this last phase, the VM on the source ESXi host is suspended and the checkpoint data is sent to the destination host. Remember that we want the switch-over time (stun time) to be 1 second or less. For large VMs, this has become a challenge because of workload sizes that have increased over time.</p>
<p>In the switch-over phase, we send over the checkpoint data and the memory bitmap. The memory bitmap is used to track all the memory of a VM. It know what pages are overwritten and still need to be transmitted the destination ESXi host. As vMotion transfers the last memory pages, the VM on the destination host begins to power on. But it might still need the last pages left for transmission. To identity these pages on the destination, we use the bitmap that is transferred from the source. If customers overcommit memory, the swapped-out pages are tracked in the optional swap bitmap.</p>
<p><img src="https://blogs.vmware.com/vsphere/files/2020/03/fullbitmapv2.png" alt="full bitmap"></p>
<h3 id="How-vSphere-7-does-it"><a href="#How-vSphere-7-does-it" class="headerlink" title="How vSphere 7 does it"></a>How vSphere 7 does it</h3><p>With VMs running 6TB of memory (or potentially even more in the future), the bitmap is already 192MB. To stay under 1 sec of switch-over time, we need the bitmap to be smaller as sending over 192MB could take up to a second or more. What if we could compact the bitmap, and only send over the information that you really need?</p>
<p><img src="http://blogs.vmware.com/vsphere/files/2020/03/compactedbitmapv2.png" alt="vSphere 7 compacted bitmap"></p>
<p>At this stage we have already copied most of the memory pages, so only the last remaining memory pages need to be sent. Using a compacted memory bitmap, vMotion is able to send bitmaps for large VMs over in milliseconds, drastically lowering the stun time.</p>
<h3 id="Performance-Improvements"><a href="#Performance-Improvements" class="headerlink" title="Performance Improvements"></a>Performance Improvements</h3><p>These enhancements to vMotion in vSphere 7 allow workloads to be live-migrated with almost no performance degradation during a vMotion. The following diagram is an example of a test to show you the potential performance gains in vSphere 7. The testbed is a large VM (72 vCPU &#x2F; 512GB) running a HammerDB workload. We monitor the commits&#x2F;sec over a timeline with a 1 second granularity.</p>
<p>A couple of key takeaways that we noticed during this test in vSphere 7, compared to vSphere 6.7, are:</p>
<ul>
<li>We no longer experience the performance impact during the page trace phase.</li>
<li>The stun time remains within 1 second instead of taking multiple seconds.</li>
<li>The overall live-migration time is almost 20 seconds shorter.</li>
</ul>
<p><img src="https://blogs.vmware.com/vsphere/files/2020/03/vmotion7-improvements.png" alt="vSphere6.7 vSphere7 "></p>
<p>Your mileage may vary depending on vMotion network configurations and VM sizing, but this is an exemplary test to show the potential benefits of these vMotion improvements.</p>
<h3 id="To-Conclude"><a href="#To-Conclude" class="headerlink" title="To Conclude"></a>To Conclude</h3><p>The improvements made in vMotion with vSphere 7 are enormous and greatly reduce the cost paid for a vMotion. You don’t need to anything to enjoy the new and improved vMotion logic, other than to upgrade your systems to vSphere 7.</p>
<p>参考：<a target="_blank" rel="noopener" href="https://blogs.vmware.com/vsphere/2020/03/vsphere-7-vmotion-enhancements.html">https://blogs.vmware.com/vsphere/2020/03/vsphere-7-vmotion-enhancements.html</a></p>
<h2 id="vSphere-7-Storage-vMotion-Improvements"><a href="#vSphere-7-Storage-vMotion-Improvements" class="headerlink" title="vSphere-7-Storage vMotion Improvements"></a>vSphere-7-Storage vMotion Improvements</h2><p>参考： </p>
<p><a target="_blank" rel="noopener" href="https://blogs.vmware.com/vsphere/2020/06/vsphere-7-storage-vmotion-improvements.html">https://blogs.vmware.com/vsphere/2020/06/vsphere-7-storage-vmotion-improvements.html</a></p>
<h2 id="ESXi提升vMotion速率-，-vMotion的属性设置"><a href="#ESXi提升vMotion速率-，-vMotion的属性设置" class="headerlink" title="ESXi提升vMotion速率 ， vMotion的属性设置"></a>ESXi提升vMotion速率 ， vMotion的属性设置</h2><p>Here’s the situation I was facing. Two Vmware vCenter 7.0.3. clusters on opposite sides of the country, connected via a a 10gig point to point ethernet circuit, and that circuit tends to have a bit of packet loss (0.5%-1%) under heavy utilization. Being cross country, it of course also has latency, typically in 55ms RTT range.</p>
<p>Vmware supports long distance vMotion. They claim you can do it with latency as high as 150ms. I found that to be the case, but whether a zero-loss site to site VPN across the internet, or across this high speed point to point link, I was never able to get vMotion to average more than a few hundred megabits. That is of course complete garbage if you’re trying to move terabytes of VM’s without downtime. If they’re frequently changing, and the rate of change exceeds the speed at which you can move data, you may even find vMotion impossible to use.</p>
<p>I found the above was due to the packet loss. I’ve done the same tests across a proper 10gig wave between locations, with the same latency but without the lossiness. On those I’d see a gigabit or two, but still far from full utilization. I suspect I could get that up to several gigabits after what I’ve learned and will share here, I just didn’t go back to test.</p>
<p>The cause of the problem is the vMotion (hot migrations) and provisioning (cold migrations) network stacks are based on decades out-of-date TCP configurations. They use a choice of two ancient congestion control algorithms, New Reno or Cubic, and both of those treat loss as congestion. When they see loss, they overreact and greatly reduce the TCP window size, causing your throughput to nosedive, then they take an excruciatingly long time to open it back up, and chances are another bit of loss will occur, compounding the issue.</p>
<p>Across this same lossy link, I did testing between two linux nodes set to use the BBR congestion control algorithm, and some other minor tuning I found at <a target="_blank" rel="noopener" href="https://fasterdata.es.net/host-tuning/linux/test-measurement-host-tuning/">https://fasterdata.es.net/host-tuning/linux/test-measurement-host-tuning/</a>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net.core.rmem_max = 134217728</span><br><span class="line">net.core.wmem_max = 134217728</span><br><span class="line">net.ipv4.tcp_rmem = 4096 87380 67108864</span><br><span class="line">net.ipv4.tcp_wmem = 4096 65536 67108864</span><br><span class="line">net.ipv4.tcp_mtu_probing = 1</span><br><span class="line">net.core.default_qdisc=fq</span><br><span class="line">net.ipv4.tcp_congestion_control=bbr</span><br></pre></td></tr></table></figure>

<p>With that config in place, I can frequently push 7+ Gbps with a single TCP session, where with the default linux congestion control of cubic, and default settings, I’d be lucky to average more than a gigabit.</p>
<p>Vmware doesn’t allow you to tinker with congestion control or windowing &#x2F; buffer settings. So my workarounds for this were focused mostly on multi-stream vmotion, after reading pages like these:</p>
<p><a target="_blank" rel="noopener" href="https://core.vmware.com/resource/how-tune-vmotion-lower-migration-times#section1">https://core.vmware.com/resource/how-tune-vmotion-lower-migration-times#section1</a><br><a target="_blank" rel="noopener" href="https://kb.vmware.com/s/article/2007467">https://kb.vmware.com/s/article/2007467</a><br>I tinkered extensively with the number of vmotion vmkernel ports defined, and custom values in the following settings:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Migrate.NetExpectedLineRateMBps</span><br><span class="line">Migrate.VMotionStreamHelpers</span><br><span class="line">Net.TcpipRxDispatchQueues</span><br><span class="line">Migrate.BindToVmknic</span><br></pre></td></tr></table></figure>

<p>I probably went through about 30 permutations of settings, and the one that I finally arrived at was:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Two target vmkernel ports</span><br><span class="line">Five <span class="built_in">source</span> vmkernel ports</span><br><span class="line"></span><br><span class="line">Migrate.NetExpectedLineRateMBps = 2000</span><br><span class="line">Net.TcpipRxDispatchQueues = 5</span><br><span class="line">Migrate.BindToVmknic = 2</span><br><span class="line">Migrate.VMotionStreamHelpers = 48</span><br></pre></td></tr></table></figure>

<p>I know that sounds kind of insane to have 48 stream helpers, but I went all the way to 64 in +8 jumps and 48 had the highest throughput for my 16 pCore source vMotion server. With the above config, 55ms 10gig circuit with ~1% loss, I was able to do cross country live vmotions of both VM state and storage at upwards of 2.4 Gbps. I did start out with more vmkernel ports on the target, but for whatever reason, while this config allows for 48 TCP sessions spread evenly across the five source vmkernel ports, they’d still only target two unique IP addresses, so the other three served no purpose.</p>
<p>This above config is also particularly useful if you have a Cogent point to point &#x2F; metroE circuit between locations, because those are often rate limited to around 2 gbps per TCP session even if you’re paying for a full 10gig circuit. Since vMotion spreads the flows evenly across all the TCP sessions, this will let you better utilize the link with no one session hitting the rate limit and being throttled.</p>
<p>Next up, I found a second way to get around this issue which could be of interest to those who want more of a bandaid fix to evacuate one cluster for another; i.e. not permanent. I happened across this reddit thread where u&#x2F;LatinSuD reported having similar issues solved by the use of socat to proxy the TCP session to the remote host. This was intriguing for me because I know my lossy link, in the hands of a modern TCP tuning, is capable of nearly wire speed. I tested it out and it worked. With only crude tuning, I was able to achieve 4 Gbps of vMotion between the same hosts and across the same link, where I could only get 2.4 Gbps with native tuning options. There’s probably more to go too if socat were running on a more modern system, but I have it on an old four core Xeon E3-1225 dating to 2011, so truly ancient hardware. The system has a Mellanox Connect-X 3 10gig NIC.</p>
<p>Here was my config:</p>
<p>Source ESXi host vmotion vmkernel interface: 10.88.0.9&#x2F;24<br>Source ESXi host vmotion network default gateway override: 10.88.0.1<br>Target ESXi host vmotion vmkernel interface: 10.99.0.9&#x2F;24<br>socat proxy linux system vmotion interface: 10.88.0.1</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">One target vmkernel port</span><br><span class="line">One <span class="built_in">source</span> vmkernel port</span><br><span class="line"></span><br><span class="line">Migrate.NetExpectedLineRateMBps = 2000</span><br><span class="line">Net.TcpipRxDispatchQueues = 5</span><br><span class="line">Migrate.BindToVmknic = 2</span><br><span class="line">Migrate.VMotionStreamHelpers = 5</span><br></pre></td></tr></table></figure>

<p>The above config results in the source vmotion server wanting to talk to port 8000 on the target 10.99.0.9. However, since it’s been given a default gateway (you could also add a custom static route if needed) of the socat server, it’s going to send its packets to that system regardless.</p>
<p>On the socat server, I alter the packets with a destination NAT rule so it delivers those packets to itself instead of discarding or attempting to route them (if forwarding were enabled):</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A PREROUTING -p tcp -d 10.99.0.9 --dport 8000 -j DNAT --to-destination 10.88.0.1:8000</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>I also have the earlier sysctl settings in place, and most importantly, BBR congestion control:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net.core.rmem_max = 134217728</span><br><span class="line">net.core.wmem_max = 134217728</span><br><span class="line">net.ipv4.tcp_rmem = 4096 87380 67108864</span><br><span class="line">net.ipv4.tcp_wmem = 4096 65536 67108864</span><br><span class="line">net.ipv4.tcp_mtu_probing = 1</span><br><span class="line">net.core.default_qdisc=fq</span><br><span class="line">net.ipv4.tcp_congestion_control=bbr</span><br></pre></td></tr></table></figure>

<p>Make sure no CPU throttling as socat is cpu-bound:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpupower frequency-set -g performance</span><br></pre></td></tr></table></figure>

<p>Now start socat up to forward the received packets to the vmotion target:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">socat TCP4-LISTEN:8000,fork,reuseaddr,<span class="built_in">bind</span>=10.88.0.1 TCP4:10.99.0.9:8000</span><br></pre></td></tr></table></figure>

<p>With the above config, the source vmotion server will use five TCP sessions. Its packets are intercepted by the socat server, rewritten to have a target of the local interface IP, socat is listening and accepts them, five socat children are spawned and connected to the target vmotion interface. It then forwards them as part of a new proxied TCP session to the target vmotion interface. Because of the proxying, the TCP conversation between source and socat occur on the local fast and non-lossy network, and the TCP conversation between the socat server and the target vmotion uses the BBR algorithm and a tuned TCP stack to achieve massively improved throughput. Like I said, old server with a 12 year old CPU and many years old NIC, still saw 4+ Gbps on this lossy link.<br><a target="_blank" rel="noopener" href="https://www.ispcolohost.com/2023/10/02/speeding-up-long-distance-vmotion/">https://www.ispcolohost.com/2023/10/02/speeding-up-long-distance-vmotion/</a></p>
<h3 id="ESXi-属性信息"><a href="#ESXi-属性信息" class="headerlink" title="ESXi 属性信息"></a>ESXi 属性信息</h3><p><a href="https://github.com/lamw/esxi-advanced-and-kernel-settings/blob/master/esxi-80a-advanced-settings.md">https://github.com/lamw/esxi-advanced-and-kernel-settings/blob/master/esxi-80a-advanced-settings.md</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/vMotion/" rel="tag"># vMotion</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/cf950940/" rel="prev" title="油猴JavaScript脚本-B站牧场物语弹幕需求">
                  <i class="fa fa-angle-left"></i> 油猴JavaScript脚本-B站牧场物语弹幕需求
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/8a2adfdb/" rel="next" title="SUSE-Linux网络配置">
                  SUSE-Linux网络配置 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Rohman.Zhu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
